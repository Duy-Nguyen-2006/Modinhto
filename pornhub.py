#!/usr/bin/env python3
"""
Pornhub Video Crawler - Educational purposes only
TÃ¬m kiáº¿m video theo tÃªn diá»…n viÃªn vá»›i 3 lá»›p lá»c
"""

import asyncio
import re
import unicodedata
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler


def normalize_name(name: str) -> str:
    """
    Chuáº©n hÃ³a tÃªn diá»…n viÃªn:
    - Loáº¡i bá» dáº¥u (accents)
    - Chuyá»ƒn thÃ nh lowercase
    - Thay space báº±ng +
    """
    # Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t vÃ  cÃ¡c dáº¥u khÃ¡c
    nfd = unicodedata.normalize('NFD', name)
    name_no_accents = ''.join(char for char in nfd if unicodedata.category(char) != 'Mn')
    
    # Lowercase vÃ  thay space báº±ng +
    normalized = name_no_accents.lower().strip()
    normalized = re.sub(r'\s+', '+', normalized)
    
    return normalized


def create_search_url(actress_name: str, page: int = 1) -> str:
    """
    Táº¡o URL search tá»« tÃªn diá»…n viÃªn Ä‘Ã£ normalize
    """
    base_url = "https://www.pornhub.com/video/search"
    normalized_name = normalize_name(actress_name)
    
    if page == 1:
        return f"{base_url}?search={normalized_name}"
    else:
        return f"{base_url}?search={normalized_name}&page={page}"


def extract_video_info(video_item, base_url: str = "https://www.pornhub.com") -> dict:
    """
    TrÃ­ch xuáº¥t thÃ´ng tin video tá»« HTML element
    Tráº£ vá» dict vá»›i keys: title, link
    """
    try:
        # TÃ¬m tháº» <a> cÃ³ attribute title (chá»©a tiÃªu Ä‘á» video)
        title_link = video_item.find('a', attrs={'title': True})
        
        if not title_link:
            return None
        
        title = title_link.get('title', '').strip()
        href = title_link.get('href', '').strip()
        
        # Lá»šP 2: Loáº¡i bá» link giáº£/khÃ´ng há»£p lá»‡
        if not href or href == '#' or 'javascript:' in href.lower():
            return None
        
        # Xá»­ lÃ½ URL (relative hoáº·c absolute)
        if href.startswith('/'):
            video_url = urljoin(base_url, href)
        elif href.startswith('http'):
            # Kiá»ƒm tra khÃ´ng pháº£i link ads Ä‘áº¿n domain khÃ¡c
            parsed = urlparse(href)
            if 'pornhub.com' not in parsed.netloc:
                return None
            video_url = href
        else:
            video_url = base_url + '/' + href
        
        return {
            'title': title,
            'link': video_url
        }
    
    except Exception as e:
        # Bá» qua video lá»—i, tiáº¿p tá»¥c
        return None


def filter_by_actress_name(videos: list, actress_name: str) -> list:
    """
    Lá»šP 3: Lá»c video theo tÃªn diá»…n viÃªn
    CHá»ˆ GIá»® video cÃ³ Ã­t nháº¥t 1 keyword cá»§a tÃªn diá»…n viÃªn trong tiÃªu Ä‘á»
    """
    # Split tÃªn diá»…n viÃªn thÃ nh keywords
    actress_keywords = actress_name.lower().split()
    
    filtered_videos = []
    
    for video in videos:
        title_lower = video['title'].lower()
        
        # Kiá»ƒm tra xem cÃ³ Ã­t nháº¥t 1 keyword trong tiÃªu Ä‘á» khÃ´ng
        has_actress_name = any(keyword in title_lower for keyword in actress_keywords)
        
        if has_actress_name:
            filtered_videos.append(video)
    
    return filtered_videos


async def search_videos_by_actor(actress_name: str, max_pages: int = 10) -> list:
    """
    Crawl vÃ  tÃ¬m kiáº¿m video theo tÃªn diá»…n viÃªn
    
    Args:
        actress_name: TÃªn diá»…n viÃªn cáº§n tÃ¬m
        max_pages: Sá»‘ trang tá»‘i Ä‘a cáº§n crawl (máº·c Ä‘á»‹nh 10)
    
    Returns:
        List cÃ¡c video dict vá»›i keys: title, link
    """
    all_videos = []
    seen_titles = set()  # Deduplication
    base_url = "https://www.pornhub.com"
    
    print(f"\nğŸ” Äang tÃ¬m kiáº¿m video cá»§a: {actress_name}")
    print(f"ğŸ“„ Sáº½ crawl tá»‘i Ä‘a {max_pages} trang káº¿t quáº£...\n")
    
    async with AsyncWebCrawler(verbose=False, headless=True) as crawler:
        current_page = 1
        
        while current_page <= max_pages:
            search_url = create_search_url(actress_name, current_page)
            
            print(f"ğŸ“¥ Äang crawl trang {current_page}: {search_url}")
            
            try:
                # Crawl trang
                result = await crawler.arun(
                    url=search_url,
                    bypass_cache=True,
                    delay_before_return_html=3.0,
                    wait_for="css:li.videoBox"
                )
                
                if not result.success:
                    print(f"âŒ Lá»—i khi crawl trang {current_page}: {result.error_message}")
                    break
                
                # Parse HTML
                soup = BeautifulSoup(result.html, 'lxml')
                
                # TÃ¬m táº¥t cáº£ video items
                video_boxes = soup.find_all('li', class_='videoBox')
                
                if not video_boxes:
                    print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y video nÃ o á»Ÿ trang {current_page}")
                    break
                
                print(f"   TÃ¬m tháº¥y {len(video_boxes)} video items")
                
                page_videos = []
                
                for video_box in video_boxes:
                    # Lá»šP 1: Loáº¡i bá» Premium Videos & Ads
                    # Kiá»ƒm tra class cÃ³ premium, sponsored, ads khÃ´ng
                    classes = video_box.get('class', [])
                    class_str = ' '.join(classes).lower()
                    
                    if any(marker in class_str for marker in ['premium', 'sponsor', 'ad-']):
                        continue
                    
                    # Kiá»ƒm tra style="display: block" (thÆ°á»ng lÃ  ads)
                    style = video_box.get('style', '')
                    if 'display' in style and 'block' in style:
                        # Note: CÃ³ thá»ƒ lÃ  video há»£p lá»‡, cáº§n kiá»ƒm tra thÃªm
                        pass
                    
                    # TrÃ­ch xuáº¥t thÃ´ng tin video
                    video_info = extract_video_info(video_box, base_url)
                    
                    if video_info:
                        # Deduplication
                        if video_info['title'] not in seen_titles:
                            seen_titles.add(video_info['title'])
                            page_videos.append(video_info)
                
                print(f"   Sau khi lá»c lá»›p 1 & 2: {len(page_videos)} video")
                
                # ThÃªm vÃ o káº¿t quáº£ tá»•ng
                all_videos.extend(page_videos)
                
                # Kiá»ƒm tra xem cÃ³ trang tiáº¿p theo khÃ´ng
                pagination = soup.find('div', class_='pagination3')
                if pagination:
                    next_page_link = pagination.find('li', class_='page_next')
                    if not next_page_link or next_page_link.find('a', href=True) is None:
                        print(f"âœ“ ÄÃ£ Ä‘áº¿n trang cuá»‘i cÃ¹ng")
                        break
                else:
                    # KhÃ´ng cÃ³ pagination, chá»‰ cÃ³ 1 trang
                    print(f"âœ“ Chá»‰ cÃ³ 1 trang káº¿t quáº£")
                    break
                
                current_page += 1
                
                # Delay giá»¯a cÃ¡c request Ä‘á»ƒ trÃ¡nh bá»‹ block
                await asyncio.sleep(2)
            
            except Exception as e:
                print(f"âŒ Lá»—i khi xá»­ lÃ½ trang {current_page}: {str(e)}")
                break
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ video crawl Ä‘Æ°á»£c (trÆ°á»›c lá»c tÃªn): {len(all_videos)}")
    
    # Lá»šP 3: Filter theo tÃªn diá»…n viÃªn (QUAN TRá»ŒNG NHáº¤T)
    filtered_videos = filter_by_actress_name(all_videos, actress_name)
    
    print(f"ğŸ“Š Sau khi lá»c theo tÃªn diá»…n viÃªn: {len(filtered_videos)} video\n")
    
    return filtered_videos


def display_results(videos: list, actress_name: str) -> None:
    """
    Hiá»ƒn thá»‹ káº¿t quáº£ tÃ¬m kiáº¿m Ä‘áº¹p máº¯t
    """
    print("=" * 80)
    print(f"ğŸ“¹ Káº¾T QUáº¢ TÃŒM KIáº¾M: {actress_name.upper()}")
    print("=" * 80)
    
    if not videos:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y video nÃ o phÃ¹ há»£p!")
        print("ğŸ’¡ Gá»£i Ã½:")
        print("   - Kiá»ƒm tra láº¡i chÃ­nh táº£ tÃªn diá»…n viÃªn")
        print("   - Thá»­ vá»›i tÃªn khÃ¡c (tÃªn tháº­t, stage name...)")
        print("   - Trang web cÃ³ thá»ƒ Ä‘ang cháº·n crawler\n")
        return
    
    print(f"\nâœ… TÃ¬m tháº¥y {len(videos)} video:\n")
    
    for i, video in enumerate(videos, 1):
        print(f"{i}. ğŸ“º {video['title']}")
        print(f"   ğŸ”— {video['link']}\n")
    
    print("=" * 80)


async def main():
    """
    ChÆ°Æ¡ng trÃ¬nh chÃ­nh
    """
    print("\n" + "=" * 80)
    print("ğŸ¬ PORNHUB VIDEO CRAWLER - BY ACTRESS NAME")
    print("=" * 80)
    print("âš ï¸  Educational purposes only - Use responsibly")
    print("=" * 80 + "\n")
    
    # Nháº­p tÃªn diá»…n viÃªn
    actress_name = input("ğŸ‘¤ Nháº­p tÃªn diá»…n viÃªn (vÃ­ dá»¥: melody marks): ").strip()
    
    if not actress_name:
        print("âŒ Vui lÃ²ng nháº­p tÃªn diá»…n viÃªn!")
        return
    
    # Máº·c Ä‘á»‹nh crawl 10 trang
    max_pages = 10
    
    # Crawl vÃ  lá»c video
    videos = await search_videos_by_actor(actress_name, max_pages)
    
    # Hiá»ƒn thá»‹ káº¿t quáº£
    display_results(videos, actress_name)


if __name__ == "__main__":
    asyncio.run(main())
