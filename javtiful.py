#!/usr/bin/env python3
"""
Javtiful Video Crawler v3.1
TÃ¬m kiáº¿m video theo tÃªn diá»…n viÃªn vá»›i DuckDuckGo fuzzy search
"""

import re
import unicodedata
from typing import List, Dict, Optional
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import asyncio
from urllib.parse import unquote, urlparse, parse_qs


def normalize_search_query(name: str) -> str:
    """
    Chuáº©n hÃ³a tÃªn Ä‘á»ƒ tÃ¬m kiáº¿m (giá»¯ khoáº£ng tráº¯ng, chuyá»ƒn thÃ nh +)
    """
    # Loáº¡i bá» dáº¥u
    name = unicodedata.normalize('NFD', name)
    name = ''.join(char for char in name if unicodedata.category(char) != 'Mn')
    
    # Lowercase vÃ  lÃ m sáº¡ch
    name = name.lower().strip()
    name = re.sub(r'[^a-z0-9\s]', '', name)
    name = re.sub(r'\s+', '+', name)
    
    return name


async def search_actress_via_duckduckgo(actress_name: str) -> Optional[str]:
    """
    TÃ¬m kiáº¿m actress qua DuckDuckGo vÃ  láº¥y slug chÃ­nh xÃ¡c
    
    Logic:
    1. Search "javtiful + actress_name" trÃªn DuckDuckGo
    2. Láº¥y link javtiful Ä‘áº§u tiÃªn
    3. Extract slug tá»« URL
    
    Args:
        actress_name: TÃªn diá»…n viÃªn (cÃ³ thá»ƒ sai chÃ­nh táº£)
    
    Returns:
        Slug cá»§a actress, hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    search_query = f"javtiful {actress_name}"
    search_query_encoded = search_query.replace(' ', '+')
    ddg_url = f"https://html.duckduckgo.com/html/?q={search_query_encoded}"
    
    print(f"\nğŸ” Äang tÃ¬m kiáº¿m qua DuckDuckGo: {actress_name}")
    print(f"ğŸ”— URL: {ddg_url}")
    
    try:
        async with AsyncWebCrawler(verbose=False, headless=True) as crawler:
            result = await crawler.arun(
                url=ddg_url,
                bypass_cache=True,
                delay_before_return_html=2.0
            )
            
            if not result.success:
                print(f"âš ï¸  Lá»—i khi search DuckDuckGo: {result.error_message}")
                return None
            
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # DuckDuckGo HTML version uses class "result__a" for result links
            result_links = soup.find_all('a', class_='result__a')
            
            if not result_links:
                print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ nÃ o tá»« DuckDuckGo")
                return None
            
            print(f"âœ… TÃ¬m tháº¥y {len(result_links)} káº¿t quáº£ tá»« DuckDuckGo")
            
            # Parse vÃ  tÃ¬m javtiful actress links
            javtiful_actress_links = []
            
            for link in result_links:
                href = link.get('href', '')
                text = link.get_text().strip()
                
                # DuckDuckGo redirects through uddg parameter
                if 'uddg=' in href:
                    parsed = urlparse(href)
                    params = parse_qs(parsed.query)
                    
                    if 'uddg' in params:
                        actual_url = unquote(params['uddg'][0])
                        
                        # Check if it's a javtiful actress/star link
                        # Support multiple domains: .com, .to, .ru, .info, etc.
                        match = re.search(r'javtiful\.[a-z]+/(actress|star|actor)/([^/?#]+)', actual_url, re.IGNORECASE)
                        
                        if match:
                            slug = match.group(2)
                            javtiful_actress_links.append({
                                'slug': slug,
                                'url': actual_url,
                                'text': text
                            })
            
            if javtiful_actress_links:
                # Láº¥y link Ä‘áº§u tiÃªn (thÆ°á»ng lÃ  káº¿t quáº£ tá»‘t nháº¥t)
                first_result = javtiful_actress_links[0]
                slug = first_result['slug']
                
                print(f"\nâœ… TÃ¬m tháº¥y actress: {first_result['text'][:60]}")
                print(f"ğŸ¯ Slug: {slug}")
                print(f"ğŸ”— Source URL: {first_result['url']}")
                
                return slug
            else:
                print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y actress link trong káº¿t quáº£")
                return None
    
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi search: {str(e)}")
        return None


def parse_videos_from_html(html_content: str) -> List[Dict[str, str]]:
    """
    Parse HTML vÃ  láº¥y danh sÃ¡ch video
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    videos = []
    
    # Method 1: TÃ¬m theo class cÃ³ chá»©a "video"
    video_links = soup.find_all('a', class_=lambda x: x and 'video' in x.lower())
    
    for link in video_links:
        title = link.get('title', '').strip()
        if not title:
            title = link.get_text().strip()
        
        href = link.get('href', '').strip()
        
        # Chá»‰ láº¥y link video
        if href and ('/video/' in href or '/watch/' in href or '/movie/' in href):
            if href.startswith('/'):
                href = f"https://javtiful.com{href}"
            
            if title and href:
                videos.append({
                    'title': title,
                    'link': href
                })
    
    # Method 2: Fallback - tÃ¬m táº¥t cáº£ link cÃ³ /video/ hoáº·c /watch/
    if not videos:
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '').strip()
            if '/video/' in href or '/watch/' in href or '/movie/' in href:
                title = link.get('title', '') or link.get_text().strip()
                
                if href.startswith('/'):
                    href = f"https://javtiful.com{href}"
                
                if title:
                    videos.append({
                        'title': title,
                        'link': href
                    })
    
    # Loáº¡i bá» trÃ¹ng láº·p
    seen_links = set()
    unique_videos = []
    for video in videos:
        if video['link'] not in seen_links:
            seen_links.add(video['link'])
            unique_videos.append(video)
    
    return unique_videos


def get_total_pages(html_content: str) -> int:
    """
    Láº¥y tá»•ng sá»‘ trang tá»« pagination
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # TÃ¬m pagination
    pagination = soup.find('ul', class_=lambda x: x and 'pagination' in x.lower() if x else False)
    
    if not pagination:
        pagination = soup.find('div', class_=lambda x: x and 'pagination' in x.lower() if x else False)
    
    if not pagination:
        return 1
    
    # TÃ¬m táº¥t cáº£ page links
    page_links = pagination.find_all('a', href=True)
    max_page = 1
    
    for link in page_links:
        page_text = link.get_text().strip()
        
        # Extract sá»‘ tá»« text
        numbers = re.findall(r'\d+', page_text)
        if numbers:
            page_num = int(numbers[0])
            max_page = max(max_page, page_num)
        
        # Extract tá»« href
        href = link.get('href', '')
        page_match = re.search(r'[?&]page=(\d+)', href)
        if page_match:
            page_num = int(page_match.group(1))
            max_page = max(max_page, page_num)
    
    return max_page


async def crawl_actress_by_slug(slug: str) -> List[Dict[str, str]]:
    """
    Crawl táº¥t cáº£ video cá»§a actress
    """
    all_videos = []
    seen_links = set()
    
    async with AsyncWebCrawler(verbose=False, headless=True) as crawler:
        first_url = f"https://javtiful.com/actress/{slug}"
        
        print(f"\nğŸ“„ Äang crawl trang 1: {first_url}")
        
        try:
            result = await crawler.arun(
                url=first_url,
                bypass_cache=True,
                delay_before_return_html=2.0
            )
            
            # Parse videos tá»« trang 1
            videos_page1 = parse_videos_from_html(result.html)
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y video, thá»­ biáº¿n thá»ƒ slug (hina <-> hiina)
            if len(videos_page1) == 0 and result.success:
                print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y video vá»›i slug: {slug}")
                print(f"ğŸ”„ Thá»­ cÃ¡c biáº¿n thá»ƒ cá»§a slug...")
                
                # Thá»­ thay Ä‘á»•i: hiina -> hina hoáº·c ngÆ°á»£c láº¡i
                alt_slugs = []
                if 'hiina' in slug:
                    alt_slugs.append(slug.replace('hiina', 'hina'))
                elif 'hina' in slug:
                    alt_slugs.append(slug.replace('hina', 'hiina'))
                if slug == 'eimi-fukada':
                    alt_slugs.append('fukada-eimi')
                
                if alt_slugs:
                    for alt_slug in alt_slugs:
                        alt_url = f"https://javtiful.com/actress/{alt_slug}"
                        print(f"ğŸ”„ Thá»­ slug: {alt_slug}")
                        print(f"ğŸ”— URL: {alt_url}")
                        
                        result = await crawler.arun(
                            url=alt_url,
                            bypass_cache=True,
                            delay_before_return_html=2.0
                        )
                        
                        if result.success:
                            videos_page1 = parse_videos_from_html(result.html)
                            
                            if len(videos_page1) > 0:
                                # ThÃ nh cÃ´ng vá»›i slug má»›i
                                slug = alt_slug
                                first_url = alt_url
                                print(f"âœ… ThÃ nh cÃ´ng vá»›i slug: {slug}")
                                break
                            else:
                                print(f"âŒ Váº«n khÃ´ng tÃ¬m tháº¥y video vá»›i slug: {alt_slug}")
                        else:
                            print(f"âŒ Lá»—i vá»›i slug: {alt_slug}")
                    
                    if len(videos_page1) == 0:
                        return []
                else:
                    print(f"âŒ KhÃ´ng cÃ³ biáº¿n thá»ƒ slug Ä‘á»ƒ thá»­")
                    return []
            
            if len(videos_page1) == 0:
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y video nÃ o!")
                return []
            
            print(f"âœ… TÃ¬m tháº¥y {len(videos_page1)} video á»Ÿ trang 1")
            
            for video in videos_page1:
                if video['link'] not in seen_links:
                    all_videos.append(video)
                    seen_links.add(video['link'])
            
            # Láº¥y tá»•ng sá»‘ trang
            total_pages = get_total_pages(result.html)
            
            if total_pages > 1:
                print(f"ğŸ“š Tá»•ng sá»‘ trang: {total_pages}\n")
            
            # Crawl cÃ¡c trang cÃ²n láº¡i
            if total_pages > 1:
                for page_num in range(2, total_pages + 1):
                    page_url = f"https://javtiful.com/actress/{slug}?page={page_num}"
                    
                    print(f"ğŸ“„ Äang crawl trang {page_num}/{total_pages}")
                    
                    try:
                        result = await crawler.arun(
                            url=page_url,
                            bypass_cache=True,
                            delay_before_return_html=1.5
                        )
                        
                        if result.success:
                            videos_page = parse_videos_from_html(result.html)
                            print(f"âœ… TÃ¬m tháº¥y {len(videos_page)} video á»Ÿ trang {page_num}")
                            
                            for video in videos_page:
                                if video['link'] not in seen_links:
                                    all_videos.append(video)
                                    seen_links.add(video['link'])
                        else:
                            print(f"âš ï¸  Lá»—i trang {page_num}: {result.error_message}")
                    
                    except Exception as e:
                        print(f"âš ï¸  Lá»—i trang {page_num}: {str(e)}")
                    
                    await asyncio.sleep(1)
                    
        except Exception as e:
            print(f"âŒ Lá»—i: {str(e)}")
            return []
    
    return all_videos


async def search_videos_by_actor(actress_name: str) -> List[Dict[str, str]]:
    """
    TÃ¬m kiáº¿m video theo tÃªn diá»…n viÃªn
    
    Logic:
    1. Search qua DuckDuckGo vá»›i "javtiful + actress_name"
    2. Láº¥y slug tá»« káº¿t quáº£ Ä‘áº§u tiÃªn
    3. Crawl táº¥t cáº£ video
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” TÃŒM KIáº¾M: {actress_name}")
    print(f"{'='*80}")
    
    # BÆ¯á»šC 1: Search qua DuckDuckGo
    slug = await search_actress_via_duckduckgo(actress_name)
    
    if not slug:
        print("\nâŒ KhÃ´ng thá»ƒ tÃ¬m tháº¥y actress!")
        return []
    
    # BÆ¯á»šC 2: Crawl videos
    print(f"\n{'='*80}")
    print(f"ğŸ“¥ CRAWL VIDEO")
    print(f"{'='*80}")
    
    videos = await crawl_actress_by_slug(slug)
    
    return videos


def display_results(videos: List[Dict[str, str]], actress_name: str):
    """
    Hiá»ƒn thá»‹ káº¿t quáº£ tÃ¬m kiáº¿m
    """
    print("\n" + "=" * 80)
    print(f"ğŸ¬ Káº¾T QUáº¢: {actress_name.upper()}")
    print("=" * 80)
    
    if not videos:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y video!")
        print("\nğŸ’¡ Gá»£i Ã½:")
        print("  - Kiá»ƒm tra tÃªn diá»…n viÃªn")
        print("  - VÃ­ dá»¥: 'Melody Marks', 'Yui Hatano'")
        return
    
    print(f"\nâœ… TÃ¬m tháº¥y {len(videos)} video")
    print(f"ğŸ“„ Hiá»ƒn thá»‹ toÃ n bá»™ video\n")
    
    for idx in range(len(videos)):
        video = videos[idx]
        print(f"{idx + 1}. {video['title']}")
        print(f"   ğŸ”— {video['link']}")
        print()
    
    print("=" * 80)


def main():
    """
    HÃ m chÃ­nh
    """
    print("=" * 80)
    print("ğŸ¥ JAVTIFUL VIDEO CRAWLER v3.1")
    print("   âœ¨ Fuzzy Search vá»›i DuckDuckGo")
    print("=" * 80)
    print("\nğŸ“ TÃ¬m kiáº¿m video theo tÃªn diá»…n viÃªn")
    print("ğŸ’¡ Há»— trá»£ tÃªn sai chÃ­nh táº£ (vÃ­ dá»¥: 'melod mar' â†’ 'melody marks')")
    print("ğŸ’¡ VÃ­ dá»¥: Melody Marks, Yui Hatano, Eimi Fukada")
    print("-" * 80)
    
    actress_name = input("\nğŸ‘¤ Nháº­p tÃªn diá»…n viÃªn: ").strip()
    
    if not actress_name:
        print("\nâŒ Vui lÃ²ng nháº­p tÃªn diá»…n viÃªn!")
        return
    
    try:
        videos = asyncio.run(search_videos_by_actor(actress_name))
        display_results(videos, actress_name)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ÄÃ£ há»§y!")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
