#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Ä‘á»ƒ crawl video theo diá»…n viÃªn tá»« website mupvl.info
Sá»­ dá»¥ng Crawl4AI vÃ  BeautifulSoup Ä‘á»ƒ phÃ¢n tÃ­ch HTML
Há»— trá»£ phÃ¢n trang vÃ  chuáº©n hÃ³a tÃªn diá»…n viÃªn qua DuckDuckGo
"""

import re
import unicodedata
import asyncio
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse, parse_qs
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import json

# Cáº¥u hÃ¬nh
BASE_URL = "https://mupvl.info"
MAX_PAGES = 10  # Giá»›i háº¡n sá»‘ trang crawl cho má»—i diá»…n viÃªn


def normalize_name(name: str) -> str:
    """
    Chuáº©n hÃ³a tÃªn diá»…n viÃªn thÃ nh slug
    Loáº¡i bá» dáº¥u, chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng vÃ  thay tháº¿ khoáº£ng tráº¯ng báº±ng dáº¥u gáº¡ch ngang
    
    Args:
        name: TÃªn diá»…n viÃªn gá»‘c (vd: "Eimi Fukada" hoáº·c "eimu fuk")
    
    Returns:
        Slug chuáº©n hÃ³a (vd: "eimi-fukada")
    """
    # Loáº¡i bá» dáº¥u tiáº¿ng Viá»‡t vÃ  cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t
    name = unicodedata.normalize('NFKD', name)
    name = name.encode('ascii', 'ignore').decode('utf-8')
    
    # Chuyá»ƒn thÃ nh chá»¯ thÆ°á»ng vÃ  loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t
    name = re.sub(r'[^a-zA-Z0-9\s-]', '', name.lower())
    
    # Thay tháº¿ nhiá»u khoáº£ng tráº¯ng liÃªn tiáº¿p báº±ng 1 khoáº£ng tráº¯ng
    name = re.sub(r'\s+', ' ', name).strip()
    
    # Thay tháº¿ khoáº£ng tráº¯ng báº±ng dáº¥u gáº¡ch ngang
    slug = name.replace(' ', '-')
    
    return slug


async def search_actress_on_duckduckgo(query: str) -> Optional[str]:
    """
    TÃ¬m kiáº¿m tÃªn diá»…n viÃªn chuáº©n trÃªn DuckDuckGo
    
    Args:
        query: TÃªn diá»…n viÃªn (cÃ³ thá»ƒ sai chÃ­nh táº£)
    
    Returns:
        TÃªn/slug diá»…n viÃªn chuáº©n, hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
    """
    print(f"ğŸ” Äang tÃ¬m kiáº¿m '{query}' trÃªn DuckDuckGo...")
    
    # Táº¡o query tÃ¬m kiáº¿m vá»›i tá»« khÃ³a actress/JAV
    search_query = f"{query} actress JAV"
    search_url = f"https://duckduckgo.com/html/?q={search_query}"
    
    try:
        async with AsyncWebCrawler(verbose=False) as crawler:
            result = await crawler.arun(url=search_url)
            
            if not result.success:
                print(f"âš ï¸  KhÃ´ng thá»ƒ truy cáº­p DuckDuckGo")
                return None
            
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # TÃ¬m suggestion "Including results for" náº¿u cÃ³
            did_you_mean = soup.find('div', id='did_you_mean')
            if did_you_mean:
                suggested_link = did_you_mean.find('a')
                if suggested_link:
                    suggested_text = suggested_link.get_text().strip().lower()
                    # Loáº¡i bá» "actress jav" khá»i suggested text
                    suggested_text = re.sub(r'\s*(actress|jav)\s*', ' ', suggested_text, flags=re.IGNORECASE).strip()
                    if suggested_text:
                        print(f"âœ“ Gá»£i Ã½ tá»« DuckDuckGo: {suggested_text}")
                        return suggested_text
            
            # TÃ¬m cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m
            results = soup.find_all('a', class_='result__a')
            
            for link in results[:10]:  # Kiá»ƒm tra 10 káº¿t quáº£ Ä‘áº§u tiÃªn
                text = link.get_text().strip()
                
                # Extract tÃªn diá»…n viÃªn tá»« title
                # Pattern: TÃ¬m tÃªn ngÆ°á»i (cÃ¡c tá»« viáº¿t hoa liÃªn tiáº¿p)
                # VD: "Melody Hiina Marks JAV Actress" -> "Melody Hiina Marks"
                
                # CÃ¡ch 1: TÃ¬m pattern "Name JAV" hoáº·c "Name Actress"
                match = re.search(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:JAV|Actress|Porn|AV)', text)
                if match:
                    actress_name = match.group(1).lower()
                    print(f"âœ“ TÃ¬m tháº¥y: {actress_name}")
                    return actress_name
                
                # CÃ¡ch 2: Láº¥y cÃ¡c tá»« viáº¿t hoa á»Ÿ Ä‘áº§u title (trÆ°á»›c cÃ¡c tá»« khÃ³a)
                words = text.split()
                name_parts = []
                for word in words:
                    # Dá»«ng khi gáº·p tá»« khÃ³a khÃ´ng pháº£i tÃªn
                    if word.lower() in ['jav', 'actress', 'porn', 'av', 'movies', 'videos', 'star', 'idol', 'model', '-', '|']:
                        break
                    # Láº¥y cÃ¡c tá»« viáº¿t hoa (cÃ³ thá»ƒ lÃ  tÃªn)
                    if word[0].isupper() and len(word) > 1:
                        name_parts.append(word)
                
                if len(name_parts) >= 2:
                    actress_name = ' '.join(name_parts).lower()
                    print(f"âœ“ TÃ¬m tháº¥y: {actress_name}")
                    return actress_name
            
            # Náº¿u khÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p, thá»­ chuáº©n hÃ³a query gá»‘c
            print(f"âš ï¸  KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ phÃ¹ há»£p, sá»­ dá»¥ng tÃªn gá»‘c")
            return query.lower().strip()
            
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi tÃ¬m kiáº¿m: {e}")
        return query.lower().strip()


def create_actress_url(actress_name: str, page: int = 1) -> List[str]:
    """
    Táº¡o cÃ¡c URL cÃ³ thá»ƒ cÃ³ cho trang diá»…n viÃªn
    Thá»­ nhiá»u pattern khÃ¡c nhau Ä‘á»ƒ tÃ¬m URL Ä‘Ãºng
    
    Args:
        actress_name: TÃªn diá»…n viÃªn Ä‘Ã£ chuáº©n hÃ³a
        page: Sá»‘ trang (máº·c Ä‘á»‹nh lÃ  1)
    
    Returns:
        Danh sÃ¡ch cÃ¡c URL cÃ³ thá»ƒ cÃ³ (sáº¯p xáº¿p theo Ä‘á»™ Æ°u tiÃªn)
    """
    slug = normalize_name(actress_name)
    parts = slug.split('-')
    
    urls = []
    page_suffix = f"?page={page}" if page > 1 else ""
    
    # Pattern 1: TÃªn Ä‘áº§y Ä‘á»§ (vd: melody-hiina-marks)
    urls.append(f"{BASE_URL}/actresses/{slug}{page_suffix}")
    
    # Pattern 2: Náº¿u cÃ³ 3 pháº§n (First Middle Last), thá»­ bá» middle name
    if len(parts) == 3:
        # First-Last (vd: melody-marks)
        first_last = f"{parts[0]}-{parts[2]}"
        urls.append(f"{BASE_URL}/actresses/{first_last}{page_suffix}")
        
        # Last-First (vd: marks-melody)
        last_first = f"{parts[2]}-{parts[0]}"
        urls.append(f"{BASE_URL}/actresses/{last_first}{page_suffix}")
        
        # First-Middle (vd: melody-hiina) - Ã­t phá»• biáº¿n nhÆ°ng váº«n thá»­
        first_middle = f"{parts[0]}-{parts[1]}"
        urls.append(f"{BASE_URL}/actresses/{first_middle}{page_suffix}")
    
    # Pattern 3: Náº¿u cÃ³ 2 pháº§n (First Last), thá»­ Ä‘áº£o ngÆ°á»£c
    elif len(parts) == 2:
        # Last-First (vd: fukada-eimi)
        reversed_slug = f"{parts[1]}-{parts[0]}"
        urls.append(f"{BASE_URL}/actresses/{reversed_slug}{page_suffix}")
    
    # Pattern 4: Náº¿u cÃ³ 4+ pháº§n, thá»­ cÃ¡c tá»• há»£p
    elif len(parts) >= 4:
        # First-Last
        first_last = f"{parts[0]}-{parts[-1]}"
        urls.append(f"{BASE_URL}/actresses/{first_last}{page_suffix}")
        
        # Last-First
        last_first = f"{parts[-1]}-{parts[0]}"
        urls.append(f"{BASE_URL}/actresses/{last_first}{page_suffix}")
    
    return urls


async def check_url_validity(url: str, crawler) -> Tuple[bool, Optional[str]]:
    """
    Kiá»ƒm tra xem URL cÃ³ há»£p lá»‡ khÃ´ng (HTTP 200 vÃ  cÃ³ video)
    
    Args:
        url: URL cáº§n kiá»ƒm tra
        crawler: AsyncWebCrawler instance
    
    Returns:
        (valid, html) - True náº¿u há»£p lá»‡, kÃ¨m theo HTML content
    """
    try:
        result = await crawler.arun(url=url)
        
        if not result.success:
            return False, None
        
        soup = BeautifulSoup(result.html, 'html.parser')
        video_items = soup.find_all('div', class_='video-item')
        
        # Náº¿u cÃ³ Ã­t nháº¥t 1 video item thÃ¬ URL há»£p lá»‡
        if len(video_items) > 0:
            return True, result.html
        
        return False, None
        
    except Exception as e:
        return False, None


def extract_videos_from_html(html: str) -> List[Dict[str, str]]:
    """
    TrÃ­ch xuáº¥t thÃ´ng tin video tá»« HTML
    
    Args:
        html: Ná»™i dung HTML cá»§a trang
    
    Returns:
        Danh sÃ¡ch cÃ¡c video vá»›i title vÃ  link
    """
    soup = BeautifulSoup(html, 'html.parser')
    videos = []
    
    # TÃ¬m táº¥t cáº£ cÃ¡c video items
    video_items = soup.find_all('div', class_='video-item')
    
    for item in video_items:
        # TÃ¬m tháº» a Ä‘áº§u tiÃªn (chá»©a link video)
        link_tag = item.find('a', class_='video-item__thumb')
        
        if link_tag:
            video_url = link_tag.get('href', '')
            video_title = link_tag.get('title', '')
            
            # Náº¿u URL khÃ´ng Ä‘áº§y Ä‘á»§, thÃªm base URL
            if video_url and not video_url.startswith('http'):
                video_url = urljoin(BASE_URL, video_url)
            
            # Náº¿u khÃ´ng cÃ³ title tá»« thumb, thá»­ láº¥y tá»« title div
            if not video_title:
                title_div = item.find('div', class_='video-item__title')
                if title_div:
                    title_link = title_div.find('a')
                    if title_link:
                        video_title = title_link.get('title', '') or title_link.get_text(strip=True)
            
            if video_url and video_title:
                videos.append({
                    'title': video_title.strip(),
                    'link': video_url.strip()
                })
    
    return videos


def has_pagination(html: str) -> bool:
    """
    Kiá»ƒm tra xem trang cÃ³ phÃ¢n trang khÃ´ng
    
    Args:
        html: Ná»™i dung HTML cá»§a trang
    
    Returns:
        True náº¿u cÃ³ phÃ¢n trang
    """
    soup = BeautifulSoup(html, 'html.parser')
    pagenavi = soup.find('div', class_='pagenavi')
    
    if pagenavi and pagenavi.find_all('a'):
        return True
    
    return False


async def search_videos_by_actress(actress_name: str) -> List[Dict[str, str]]:
    """
    TÃ¬m kiáº¿m vÃ  crawl táº¥t cáº£ video cá»§a má»™t diá»…n viÃªn
    Há»— trá»£ phÃ¢n trang tá»± Ä‘á»™ng
    
    Args:
        actress_name: TÃªn diá»…n viÃªn Ä‘Ã£ chuáº©n hÃ³a
    
    Returns:
        Danh sÃ¡ch táº¥t cáº£ video tá»« táº¥t cáº£ cÃ¡c trang
    """
    all_videos = []
    seen_links = set()  # Äá»ƒ loáº¡i bá» trÃ¹ng láº·p
    
    print(f"\nğŸ¬ Báº¯t Ä‘áº§u crawl video cá»§a {actress_name}...")
    
    async with AsyncWebCrawler(verbose=False) as crawler:
        # BÆ°á»›c 1: TÃ¬m URL há»£p lá»‡
        print("ğŸ“ Äang tÃ¬m URL há»£p lá»‡...")
        possible_urls = create_actress_url(actress_name, page=1)
        
        valid_url = None
        base_html = None
        
        for url in possible_urls:
            print(f"   Thá»­: {url}")
            is_valid, html = await check_url_validity(url, crawler)
            
            if is_valid:
                valid_url = url
                base_html = html
                print(f"   âœ“ URL há»£p lá»‡!")
                break
        
        if not valid_url:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y trang diá»…n viÃªn há»£p lá»‡")
            return []
        
        # BÆ°á»›c 2: Crawl trang Ä‘áº§u tiÃªn
        print(f"\nğŸ“„ Crawl trang 1...")
        videos = extract_videos_from_html(base_html)
        
        for video in videos:
            if video['link'] not in seen_links:
                all_videos.append(video)
                seen_links.add(video['link'])
        
        print(f"   âœ“ TÃ¬m tháº¥y {len(videos)} video")
        
        # BÆ°á»›c 3: Kiá»ƒm tra vÃ  crawl cÃ¡c trang tiáº¿p theo
        # Láº¥y base URL (khÃ´ng cÃ³ query params)
        base_actress_url = valid_url.split('?')[0]
        
        for page_num in range(2, MAX_PAGES + 1):
            page_url = f"{base_actress_url}?page={page_num}"
            
            print(f"\nğŸ“„ Crawl trang {page_num}...")
            print(f"   URL: {page_url}")
            
            try:
                result = await crawler.arun(url=page_url)
                
                if not result.success:
                    print(f"   âš ï¸  KhÃ´ng thá»ƒ truy cáº­p trang {page_num}")
                    break
                
                videos = extract_videos_from_html(result.html)
                
                # Náº¿u khÃ´ng cÃ²n video, dá»«ng láº¡i
                if len(videos) == 0:
                    print(f"   â„¹ï¸  KhÃ´ng cÃ²n video, dá»«ng crawl")
                    break
                
                # ThÃªm video má»›i vÃ o danh sÃ¡ch
                new_videos_count = 0
                for video in videos:
                    if video['link'] not in seen_links:
                        all_videos.append(video)
                        seen_links.add(video['link'])
                        new_videos_count += 1
                
                print(f"   âœ“ TÃ¬m tháº¥y {len(videos)} video ({new_videos_count} video má»›i)")
                
                # Náº¿u khÃ´ng cÃ³ video má»›i, cÃ³ thá»ƒ Ä‘Ã£ háº¿t
                if new_videos_count == 0:
                    print(f"   â„¹ï¸  KhÃ´ng cÃ³ video má»›i, dá»«ng crawl")
                    break
                
                # Delay nhá» giá»¯a cÃ¡c request
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"   âš ï¸  Lá»—i khi crawl trang {page_num}: {e}")
                break
    
    return all_videos


def display_results(videos: List[Dict[str, str]], actress_name: str):
    """
    Hiá»ƒn thá»‹ káº¿t quáº£ tÃ¬m kiáº¿m má»™t cÃ¡ch Ä‘áº¹p máº¯t
    
    Args:
        videos: Danh sÃ¡ch video
        actress_name: TÃªn diá»…n viÃªn
    """
    print("\n" + "="*80)
    print(f"ğŸ¯ Káº¾T QUáº¢ TÃŒM KIáº¾M CHO: {actress_name.upper()}")
    print("="*80)
    
    if not videos:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y video nÃ o!")
        return
    
    print(f"\nâœ“ TÃ¬m tháº¥y tá»•ng cá»™ng {len(videos)} video\n")
    
    for idx, video in enumerate(videos, 1):
        print(f"{idx}. {video['title']}")
        print(f"   ğŸ”— {video['link']}")
        print()
    
    print("="*80)


async def main():
    """
    HÃ m chÃ­nh - cháº¡y chÆ°Æ¡ng trÃ¬nh
    """
    print("="*80)
    print("ğŸ¬ CÃ”NG Cá»¤ TÃŒM KIáº¾M VIDEO THEO DIá»„N VIÃŠN - MUPVL.INFO")
    print("="*80)
    
    # Nháº­n input tá»« ngÆ°á»i dÃ¹ng
    actress_input = input("\nğŸ‘¤ Nháº­p tÃªn diá»…n viÃªn (VD: eimi fukada, eimu fuk): ").strip()
    
    if not actress_input:
        print("âŒ Vui lÃ²ng nháº­p tÃªn diá»…n viÃªn!")
        return
    
    print(f"\nğŸ“ Báº¡n Ä‘Ã£ nháº­p: {actress_input}")
    
    # BÆ°á»›c 1: Chuáº©n hÃ³a tÃªn qua DuckDuckGo
    normalized_name = await search_actress_on_duckduckgo(actress_input)
    
    if not normalized_name:
        print("âŒ KhÃ´ng thá»ƒ chuáº©n hÃ³a tÃªn diá»…n viÃªn. Vui lÃ²ng thá»­ láº¡i!")
        return
    
    print(f"âœ“ TÃªn chuáº©n: {normalized_name}")
    
    # BÆ°á»›c 2: TÃ¬m kiáº¿m video
    videos = await search_videos_by_actress(normalized_name)
    
    # BÆ°á»›c 3: Hiá»ƒn thá»‹ káº¿t quáº£
    display_results(videos, normalized_name)


if __name__ == "__main__":
    # Cháº¡y chÆ°Æ¡ng trÃ¬nh
    asyncio.run(main())
